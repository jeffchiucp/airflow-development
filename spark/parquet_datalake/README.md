dependency: python/scheduling_challenge

### Part 1
1. Change all past JSON and/or CSV files stored into parquet files, resolving all datatype issues that may arise
2. Update the `python/scheduling_challenge` to convert all CSV or JSON files into parquet files

### Part 2
1. Compare and contrast the computational speed of two scripts: one leveraging pandas+csv/json and the other pyspark+parquet.
2. Much like `spark/pyspark_computational` challenge, pick a few simple data transformations to benchmark time to complete.


Extra reading: https://angelddaz.substack.com/p/building-a-robust-data-lake-with
